{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "import sys\n",
    "from functools import partial\n",
    "import json\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import circuitsvis as cv\n",
    "import webbrowser\n",
    "from IPython.display import display\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    ActivationCache,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    ")\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.components import LayerNorm\n",
    "from eindex import eindex\n",
    "\n",
    "# t.set_grad_enabled(False)\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformers\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"monthly_algorithmic_problems\" / \"october23_sorted_list\"\n",
    "if str(exercises_dir) not in sys.path:\n",
    "    sys.path.append(str(exercises_dir))\n",
    "\n",
    "from monthly_algorithmic_problems.october23_sorted_list.model import create_model\n",
    "from monthly_algorithmic_problems.october23_sorted_list.training import train, TrainArgs\n",
    "from monthly_algorithmic_problems.october23_sorted_list.dataset import SortedListDataset\n",
    "from plotly_utils import hist, bar, imshow\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SortedListDataset(size=10, list_len=5, max_value=15, seed=43)\n",
    "\n",
    "print(\"Sequence = \", dataset[0])\n",
    "print(\"Str toks = \", dataset.str_toks[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-layer transformer, 3 heads, no MLP\n",
    "\n",
    "args = TrainArgs(\n",
    "    list_len=10,\n",
    "    max_value=50,\n",
    "    trainset_size=150_000,\n",
    "    valset_size=10_000,\n",
    "    epochs=25,\n",
    "    batch_size=512,\n",
    "    lr_start=1e-3,\n",
    "    lr_end=1e-4,\n",
    "    weight_decay=0.005,\n",
    "    seed=42,\n",
    "    d_model=96,\n",
    "    d_head=48,\n",
    "    n_layers=1,\n",
    "    n_heads=2,\n",
    "    d_mlp=None,\n",
    "    normalization_type=\"LN\",\n",
    "    use_wandb=False,\n",
    "    device=device,\n",
    ")\n",
    "model = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SortedListDataset(size=500, list_len=10, max_value=50, seed=43)\n",
    "\n",
    "logits, cache = model.run_with_cache(dataset.toks)\n",
    "logits: Tensor = logits[:, dataset.list_len : -1, :]\n",
    "\n",
    "targets = dataset.toks[:, dataset.list_len + 1 :]\n",
    "\n",
    "logprobs = logits.log_softmax(-1)  # [batch seq_len vocab_out]\n",
    "probs = logprobs.softmax(-1)\n",
    "\n",
    "batch_size, seq_len = dataset.toks.shape\n",
    "logprobs_correct = eindex(logprobs, targets, \"batch seq [batch seq]\")\n",
    "probs_correct = eindex(probs, targets, \"batch seq [batch seq]\")\n",
    "\n",
    "avg_cross_entropy_loss = -logprobs_correct.mean().item()\n",
    "\n",
    "print(f\"Average cross entropy loss: {avg_cross_entropy_loss:.3f}\")\n",
    "print(f\"Mean probability on correct label: {probs_correct.mean():.3f}\")\n",
    "print(f\"Median probability on correct label: {probs_correct.median():.3f}\")\n",
    "print(f\"Min probability on correct label: {probs_correct.min():.3f}\")\n",
    "\n",
    "cv.attention.from_cache(\n",
    "    cache=cache,\n",
    "    tokens=dataset.str_toks,\n",
    "    batch_idx=list(range(10)),\n",
    "    radioitems=True,\n",
    "    return_mode=\"view\",\n",
    "    batch_labels=[\" \".join(s) for s in dataset.str_toks],\n",
    "    mode=\"small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = section_dir / \"sorted_list_model.pt\"\n",
    "t.save(model.state_dict(), filename)\n",
    "\n",
    "# Check we can load in the model\n",
    "model_loaded = create_model(\n",
    "    list_len=10,\n",
    "    max_value=50,\n",
    "    seed=0,\n",
    "    d_model=96,\n",
    "    d_head=48,\n",
    "    n_layers=1,\n",
    "    n_heads=2,\n",
    "    normalization_type=\"LN\",\n",
    "    d_mlp=None,\n",
    ")\n",
    "model_loaded.load_state_dict(t.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(i):\n",
    "    imshow(\n",
    "        probs[i].T,\n",
    "        y=dataset.vocab,\n",
    "        x=[\n",
    "            f\"{dataset.str_toks[i][j]}<br><sub>({j})</sub>\"\n",
    "            for j in range(dataset.list_len + 1, dataset.seq_len)\n",
    "        ],\n",
    "        labels={\"x\": \"Token\", \"y\": \"Vocab\"},\n",
    "        xaxis_tickangle=0,\n",
    "        title=f\"Sample model probabilities:<br>Unsorted = ({','.join(dataset.str_toks[i][:dataset.list_len])})\",\n",
    "        text=[\n",
    "            [\n",
    "                \"ã€‡\" if (str_tok == target) else \"\"\n",
    "                for target in dataset.str_toks[i][\n",
    "                    dataset.list_len + 1 : dataset.seq_len\n",
    "                ]\n",
    "            ]\n",
    "            for str_tok in dataset.vocab\n",
    "        ],\n",
    "        width=400,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "\n",
    "show(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
